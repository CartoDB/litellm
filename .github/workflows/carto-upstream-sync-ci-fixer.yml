name: CARTO Upstream Sync - CI Failure Auto-Fix

# This workflow automatically fixes CI failures in upstream sync PRs.
# Triggered when CI checks fail on PRs created by the upstream sync workflow
# or the conflict resolver.

on:
  workflow_run:
    workflows:
      - "LiteLLM Mock Tests (folder - tests/test_litellm)"
      - "CARTO - Deploy Docker Image (CI)"
      - "LiteLLM Linting"
    types:
      - completed
    branches:
      - 'upstream-sync/**'
      - 'upstream-sync-resolver/**'

  workflow_dispatch:
    inputs:
      pr-number:
        description: "PR number to fix CI for"
        required: true
        default: ""

permissions:
  contents: write
  pull-requests: write
  actions: read

# Prevent multiple workflows running on the same PR simultaneously
# IMPORTANT: Uses shared concurrency group with Resolver to prevent both running simultaneously
concurrency:
  group: upstream-sync-${{ github.event.workflow_run.pull_requests[0].number || github.event.inputs.pr-number || github.run_id }}
  cancel-in-progress: false  # Wait for running workflow to finish (don't cancel)

jobs:
  check-ci-status:
    name: Check CI Status
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'failure' || github.event_name == 'workflow_dispatch'

    outputs:
      should-fix: ${{ steps.check.outputs.should-fix }}
      pr-number: ${{ steps.check.outputs.pr-number }}
      branch-name: ${{ steps.check.outputs.branch-name }}

    steps:
      - name: Get PR information
        id: check
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          echo "::group::Determining PR and branch"

          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            # Manual trigger
            PR_NUMBER="${{ github.event.inputs.pr-number }}"
            PR_JSON=$(gh pr view ${PR_NUMBER} --repo ${{ github.repository }} --json headRefName,labels)
            BRANCH_NAME=$(echo "${PR_JSON}" | jq -r '.headRefName')
          else
            # Automatic trigger from workflow_run
            BRANCH_NAME="${{ github.event.workflow_run.head_branch }}"

            # Find PR for this branch
            PR_JSON=$(gh pr list \
              --repo ${{ github.repository }} \
              --head "${BRANCH_NAME}" \
              --json number,labels \
              --jq '.[0]')

            if [[ "${PR_JSON}" == "null" || -z "${PR_JSON}" ]]; then
              echo "[Fixer] âŒ No PR found for branch ${BRANCH_NAME}"
              echo "should-fix=false" >> $GITHUB_OUTPUT
              exit 0
            fi

            PR_NUMBER=$(echo "${PR_JSON}" | jq -r '.number')
          fi

          # Use jq for exact label matching (avoid substring matches)
          LABELS=$(echo "${PR_JSON}" | jq -r '.labels[].name' | tr '\n' ', ' | sed 's/, $//')
          HAS_CONFLICT_RESOLUTION=$(echo "${PR_JSON}" | jq '[.labels[].name] | any(. == "conflict-resolution")')
          HAS_AUTOMATED=$(echo "${PR_JSON}" | jq '[.labels[].name] | any(. == "automated")')
          HAS_UPSTREAM_SYNC=$(echo "${PR_JSON}" | jq '[.labels[].name] | any(. == "upstream-sync")')

          echo "[Fixer] Branch: ${BRANCH_NAME}"
          echo "[Fixer] PR: #${PR_NUMBER}"
          echo "[Fixer] Labels: ${LABELS}"

          # Only fix PRs with upstream sync related labels (exact match)
          if [[ "${HAS_CONFLICT_RESOLUTION}" != "true" && "${HAS_AUTOMATED}" != "true" && "${HAS_UPSTREAM_SYNC}" != "true" ]]; then
            echo "[Fixer] âŒ PR doesn't have required labels (need: conflict-resolution, automated, or upstream-sync) - skipping"
            echo "should-fix=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Only fix upstream sync related branches
          if [[ ! "${BRANCH_NAME}" =~ ^upstream-sync-resolver/ && ! "${BRANCH_NAME}" =~ ^upstream-sync/ ]]; then
            echo "[Fixer] âŒ Not an upstream sync branch (need: upstream-sync-resolver/* or upstream-sync/*) - skipping"
            echo "should-fix=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "should-fix=true" >> $GITHUB_OUTPUT
          echo "pr-number=${PR_NUMBER}" >> $GITHUB_OUTPUT
          echo "branch-name=${BRANCH_NAME}" >> $GITHUB_OUTPUT

          echo "::endgroup::"

  fix-ci-failures:
    name: Fix CI Failures with Claude
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: check-ci-status
    if: needs.check-ci-status.outputs.should-fix == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ needs.check-ci-status.outputs.branch-name }}
          fetch-depth: 0
          token: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}

      - name: Configure git
        run: |
          git config --global user.name "Cartofante"
          git config --global user.email "cartofante@carto.com"

      - name: Get failed workflow details
        id: failures
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          echo "::group::Analyzing CI failures"

          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"

          # Get PR head SHA
          HEAD_SHA=$(gh pr view ${PR_NUMBER} --repo ${{ github.repository }} --json headRefOid --jq '.headRefOid')

          # Get all failed workflow runs for this commit
          gh run list \
            --repo ${{ github.repository }} \
            --commit ${HEAD_SHA} \
            --status failure \
            --json databaseId,name,url \
            --limit 10 > /tmp/failed_runs.json

          FAILED_COUNT=$(cat /tmp/failed_runs.json | jq 'length')

          echo "[Fixer] Found ${FAILED_COUNT} failed workflow runs"
          cat /tmp/failed_runs.json | jq -r '.[] | "  - \(.name): \(.url)"'

          # Get check runs details (gh pr checks uses 'state' not 'conclusion')
          gh pr checks ${PR_NUMBER} \
            --repo ${{ github.repository }} \
            --json name,state,link \
            > /tmp/check_runs.json

          FAILED_CHECKS=$(cat /tmp/check_runs.json | jq '[.[] | select(.state == "FAILURE" or .state == "TIMED_OUT")]')
          echo "${FAILED_CHECKS}" > /tmp/failed_checks.json

          echo "[Fixer] Failed checks:"
          echo "${FAILED_CHECKS}" | jq -r '.[] | "  - \(.name): \(.state)"'

          echo "failed-count=${FAILED_COUNT}" >> $GITHUB_OUTPUT
          echo "head-sha=${HEAD_SHA}" >> $GITHUB_OUTPUT

          echo "::endgroup::"

      - name: Download workflow logs
        if: steps.failures.outputs.failed-count > 0
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          echo "::group::Downloading workflow logs"

          mkdir -p /tmp/workflow-logs

          # Download logs for each failed run
          cat /tmp/failed_runs.json | jq -r '.[] | .databaseId' | while read RUN_ID; do
            echo "[Fixer] ðŸ“¥ Downloading logs for run ${RUN_ID}..."
            gh run download ${RUN_ID} \
              --repo ${{ github.repository }} \
              --dir /tmp/workflow-logs/${RUN_ID} 2>/dev/null || true
          done

          # List downloaded logs
          echo "[Fixer] Downloaded logs:"
          find /tmp/workflow-logs -type f -name "*.txt" | head -20

          echo "::endgroup::"

      - name: Comment on PR - Starting Fix
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"

          FAILED_CHECKS=$(cat /tmp/failed_checks.json | jq -r '.[] | "- **\(.name)**: \(.state)"' | head -10)

          gh pr comment ${PR_NUMBER} \
            --repo ${{ github.repository }} \
            --body "$(cat <<EOF
          ## ðŸ”§ CI Auto-Fix Started

          **Status:** â³ In progress...

          Claude Code (Opus 4.5) is analyzing and fixing CI failures.

          **Failed Checks:**
          ${FAILED_CHECKS}

          | Step | Status |
          |------|--------|
          | ðŸ” Analyze failures | In progress |
          | âœï¸ Apply fixes | Pending |
          | ðŸ“Œ Push fixes | Pending |

          > [!NOTE]
          > This may take **15-30 minutes**. Fixes will be pushed directly to this PR.

          [View workflow run â†’](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          EOF
          )"

      - name: Setup GCP credentials for Vertex AI
        run: |
          echo '${{ secrets.CI_RESOURCES_SERVICE_ACCOUNT }}' > /tmp/gcp-sa.json
          echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-sa.json" >> $GITHUB_ENV

      - name: Fetch PR comments for context
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          echo "::group::Fetching PR comments for previous fix context"

          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"

          # Fetch all PR comments (includes previous fix attempts, human feedback)
          gh pr view ${PR_NUMBER} \
            --repo ${{ github.repository }} \
            --json comments \
            --jq '.comments[] | "---\n**\(.author.login)** at \(.createdAt):\n\(.body)\n"' \
            > /tmp/pr_comments.md 2>/dev/null || echo "No comments found" > /tmp/pr_comments.md

          COMMENT_COUNT=$(gh pr view ${PR_NUMBER} --repo ${{ github.repository }} --json comments --jq '.comments | length' 2>/dev/null || echo "0")
          echo "[CI Fixer] Found ${COMMENT_COUNT} PR comments for context"

          echo "::endgroup::"

      - name: Run Claude Code to fix failures
        id: claude-fix
        uses: anthropics/claude-code-action@v1
        with:
          use_vertex: "true"
          github_token: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
          show_full_output: true
          claude_args: "--model claude-opus-4-5@20251101 --max-turns 200 --allowedTools Read,Write,Edit,Bash,Grep,Glob"
          prompt: |
            # CI Failure Auto-Fix - Iterative Resolution

            You are an automated CI fixer for PR #${{ needs.check-ci-status.outputs.pr-number }}.
            Branch: `${{ needs.check-ci-status.outputs.branch-name }}`

            **YOUR MISSION:** Fix ALL failing CI checks (Mock Tests AND Docker Build) until they pass.
            This workflow runs iteratively - if your fix doesn't work, you'll run again with updated logs.

            ---

            ## âš ï¸ CRITICAL RULES

            1. **NEVER revert CARTO customizations** - they are intentional
            2. **Fix tests to match CARTO behavior**, not vice versa
            3. **Check ACTUAL error logs** before making changes
            4. **Learn from previous attempts** - don't repeat failed fixes

            ---

            ## PHASE 1: ANALYZE FAILURES (DO THIS FIRST!)

            ### 1.1 Check what's currently failing

            ```bash
            echo "=== PR STATUS ==="
            gh pr checks ${{ needs.check-ci-status.outputs.pr-number }} --repo ${{ github.repository }}
            ```

            ### 1.2 Review previous fix attempts on this branch

            ```bash
            echo "=== RECENT COMMITS (previous fix attempts) ==="
            git log --oneline -15

            echo "=== FILES CHANGED IN LAST 5 COMMITS ==="
            git diff --name-only HEAD~5..HEAD 2>/dev/null || git diff --name-only HEAD~3..HEAD
            ```

            ### 1.3 Get ACTUAL errors from failed CI jobs

            **For Docker Build failures:**
            ```bash
            echo "=== DOCKER BUILD ERRORS ==="
            DOCKER_RUN=$(gh run list --repo ${{ github.repository }} --workflow="CARTO - Deploy Docker Image (CI)" --branch="${{ needs.check-ci-status.outputs.branch-name }}" --status=failure --json databaseId --jq '.[0].databaseId')
            if [ -n "$DOCKER_RUN" ]; then
              gh run view $DOCKER_RUN --repo ${{ github.repository }} --log 2>&1 | grep -E "(Type error|error TS|Failed to compile|Cannot find module|Error:|FAILED|ERROR)" | tail -50
            fi
            ```

            **For Mock Tests failures:**
            ```bash
            echo "=== MOCK TESTS ERRORS ==="
            TESTS_RUN=$(gh run list --repo ${{ github.repository }} --workflow="LiteLLM Mock Tests (folder - tests/test_litellm)" --branch="${{ needs.check-ci-status.outputs.branch-name }}" --status=failure --json databaseId --jq '.[0].databaseId')
            if [ -n "$TESTS_RUN" ]; then
              gh run view $TESTS_RUN --repo ${{ github.repository }} --log 2>&1 | grep -E "(FAILED|ERROR|Error|ImportError|ModuleNotFoundError|AssertionError|SyntaxError)" | tail -50
            fi
            ```

            ---

            ## PHASE 2: IDENTIFY ROOT CAUSE

            Based on the errors from Phase 1, categorize the failure:

            | Error Pattern | Root Cause | Fix Location |
            |--------------|------------|--------------|
            | `Type error: Expected X arguments` | TypeScript function signature mismatch | `ui/litellm-dashboard/src/` |
            | `Cannot find module` | Missing import in UI | `ui/litellm-dashboard/src/` |
            | `Failed to compile` | Next.js build error | `ui/litellm-dashboard/src/` |
            | `ImportError` / `ModuleNotFoundError` | Python import missing | `litellm/` or `tests/` |
            | `SyntaxError` | Conflict markers or bad merge | Check for `<<<<<<<` markers |
            | `FAILED tests/...` | Test assertion failure | `tests/` - fix test expectations |
            | `AssertionError` | Test expecting wrong value | Update test to match CARTO behavior |

            ---

            ## PHASE 3: FIX THE ISSUE

            ### For TypeScript/UI Errors (Docker Build)

            **UI code location:** `ui/litellm-dashboard/src/`

            1. **Find the exact error file and line** from Docker logs
            2. **Read the file** to understand the context
            3. **Find the function definition** to understand expected signature:
               ```bash
               grep -rn "export.*FUNCTION_NAME\|const FUNCTION_NAME\|function FUNCTION_NAME" ui/litellm-dashboard/src/
               ```
            4. **Fix the call site** to match the function signature
            5. **Check for similar issues** in the same file

            ### For Python Test Errors (Mock Tests)

            1. **Identify the failing test** from logs
            2. **Read the test file** to understand what it's testing
            3. **Check if CARTO changed the behavior** being tested
            4. **Fix the TEST expectations**, not the CARTO code:
               ```python
               # Update assertion to match CARTO behavior
               assert result == "carto_expected_value"

               # OR skip if testing removed functionality
               @pytest.mark.skip(reason="CARTO: Behavior changed intentionally")
               ```

            ### For Conflict Markers

            ```bash
            # Find any remaining conflict markers
            grep -rn "^<<<<<<< \|^=======\|^>>>>>>>" --include="*.py" --include="*.tsx" --include="*.ts" litellm/ tests/ ui/ 2>/dev/null
            ```

            ### For Import Errors

            ```bash
            # Check if the import exists elsewhere
            grep -rn "from.*import.*MISSING_THING\|import.*MISSING_THING" litellm/
            ```

            ---

            ## PHASE 4: VERIFY & PUSH

            ### 4.1 Verify Python fixes (if you made Python changes)

            ```bash
            # Quick syntax check
            python -m py_compile litellm/**/*.py 2>&1 | head -20 || true

            # Check for conflict markers
            grep -r "^<<<<<<< " --include="*.py" litellm/ tests/ && echo "CONFLICT MARKERS FOUND!" || echo "No conflict markers"
            ```

            ### 4.2 Commit and push your fix

            ```bash
            git add -A
            git status

            git commit -m "fix(ci): [SPECIFIC FIX DESCRIPTION]

            Error fixed: [EXACT ERROR MESSAGE]
            File: [FILE PATH]
            Change: [WHAT YOU CHANGED]

            Previous attempts: [X] commits already tried
            This fix addresses: [ROOT CAUSE]
            "

            git push origin ${{ needs.check-ci-status.outputs.branch-name }}
            ```

            ### 4.3 Comment on PR with details

            ```bash
            gh pr comment ${{ needs.check-ci-status.outputs.pr-number }} --repo ${{ github.repository }} --body "## ðŸ”§ CI Fix Applied

            **Error Fixed:**
            \`\`\`
            [PASTE EXACT ERROR]
            \`\`\`

            **Fix Applied:**
            - File: \`[FILE PATH]\`
            - Change: [DESCRIPTION]

            **Verification:** Pushed to branch, CI will re-run automatically.

            ---
            _Attempt #[N] - If this fails, I'll analyze the new errors and try again._"
            ```

            ---

            ## DECISION TREE

            ```
            START
              â”‚
              â”œâ”€â–º Check PR status (gh pr checks)
              â”‚
              â”œâ”€â–º Is Docker Build failing?
              â”‚     â”‚
              â”‚     â””â”€â–º YES: Get Docker logs â†’ Find TypeScript error â†’ Fix in ui/litellm-dashboard/
              â”‚
              â”œâ”€â–º Are Mock Tests failing?
              â”‚     â”‚
              â”‚     â””â”€â–º YES: Get test logs â†’ Find failing test â†’ Fix test expectations (not CARTO code)
              â”‚
              â”œâ”€â–º Are there conflict markers?
              â”‚     â”‚
              â”‚     â””â”€â–º YES: Remove markers, keep CARTO version
              â”‚
              â””â”€â–º Commit & Push â†’ CI re-runs â†’ If fails, this workflow runs again
            ```

            ---

            ## EXAMPLES

            ### Example 1: TypeScript Function Signature Error

            **Error:** `Type error: Expected 0 arguments, but got 1`
            **File:** `ui/litellm-dashboard/src/.../PriceDataManagementTab.tsx:28`
            **Code:** `const data = await modelCostMap(accessToken);`

            **Fix Process:**
            1. Find function definition: `grep -rn "modelCostMap" ui/litellm-dashboard/src/`
            2. Check signature: `export const modelCostMap = async (): Promise<...>`
            3. Fix call: Remove the argument â†’ `const data = await modelCostMap();`

            ### Example 2: Python Test Assertion Error

            **Error:** `AssertionError: assert 'old_value' == 'new_carto_value'`
            **File:** `tests/test_something.py::test_feature`

            **Fix Process:**
            1. CARTO intentionally changed the behavior
            2. Update test: `assert result == 'new_carto_value'`
            3. Add comment: `# CARTO: Updated for new behavior`

            ---

            ## NOW START!

            1. Run Phase 1 commands to get actual errors
            2. Identify the root cause
            3. Apply the fix
            4. Commit and push
            5. The CI will re-run - if it fails again, this workflow triggers again with new logs
        env:
          ANTHROPIC_VERTEX_PROJECT_ID: carto-ci-resources
          CLOUD_ML_REGION: global
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
          GITHUB_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}

      - name: Cleanup GCP credentials
        if: always()
        run: rm -f /tmp/gcp-sa.json

      - name: Comment on PR - Result
        if: always()
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"

          if [ "${{ steps.claude-fix.outcome }}" == "success" ]; then
            gh pr comment ${PR_NUMBER} \
              --repo ${{ github.repository }} \
              --body "$(cat <<EOF
          ## âœ… CI Fix Complete

          Fixes applied and pushed to this PR.

          | Step | Status |
          |------|--------|
          | ðŸ” Analyze failures | âœ… Complete |
          | âœï¸ Apply fixes | âœ… Complete |
          | ðŸ“Œ Push fixes | âœ… Complete |

          > [!NOTE]
          > CI will re-run automatically. Monitor check results below.

          ### Next Steps

          1. Wait for CI checks to complete
          2. If failures persist, manual review may be needed
          3. Merge when all checks pass

          <details>
          <summary>ðŸ”§ Workflow Details (click to expand)</summary>

          **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

          </details>
          EOF
          )"
          else
            gh pr comment ${PR_NUMBER} \
              --repo ${{ github.repository }} \
              --body "$(cat <<EOF
          ## âŒ CI Fix Failed

          The automated fix encountered an error.

          | Step | Status |
          |------|--------|
          | ðŸ” Analyze failures | ${{ steps.claude-fix.outcome }} |
          | âœï¸ Apply fixes | - |
          | ðŸ“Œ Push fixes | - |

          > [!WARNING]
          > Manual intervention required. Check the workflow logs below.

          <details>
          <summary>ðŸ”§ Troubleshooting (click to expand)</summary>

          **Common issues:**
          - Complex merge conflicts requiring human judgment
          - Multiple interrelated failures
          - Test failures requiring code logic changes

          **Manual fix steps:**
          1. Check the workflow logs
          2. Review error messages
          3. Fix issues locally and push

          </details>

          [View workflow logs â†’](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          EOF
          )"
          fi
