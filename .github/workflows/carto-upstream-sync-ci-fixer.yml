name: CARTO Upstream Sync - CI Failure Auto-Fix

# This workflow automatically fixes CI failures in upstream sync PRs.
# Triggered when CI checks fail on PRs created by the upstream sync workflow
# or the conflict resolver.

on:
  workflow_run:
    workflows:
      - "LiteLLM Mock Tests (folder - tests/test_litellm)"
      - "CARTO - Deploy Docker Image (CI)"
      - "LiteLLM Linting"
    types:
      - completed
    branches:
      - 'upstream-sync/**'
      - 'upstream-sync-resolver/**'

  workflow_dispatch:
    inputs:
      pr-number:
        description: "PR number to fix CI for"
        required: true
        default: ""

permissions:
  contents: write
  pull-requests: write
  actions: read

# Prevent multiple workflows running on the same PR simultaneously
# IMPORTANT: Uses shared concurrency group with Resolver to prevent both running simultaneously
concurrency:
  group: upstream-sync-${{ github.event.workflow_run.pull_requests[0].number || github.event.inputs.pr-number || github.run_id }}
  cancel-in-progress: false  # Wait for running workflow to finish (don't cancel)

jobs:
  check-ci-status:
    name: Check CI Status
    runs-on: ubuntu-latest
    if: github.event.workflow_run.conclusion == 'failure' || github.event_name == 'workflow_dispatch'

    outputs:
      should-fix: ${{ steps.check.outputs.should-fix }}
      pr-number: ${{ steps.check.outputs.pr-number }}
      branch-name: ${{ steps.check.outputs.branch-name }}

    steps:
      - name: Get PR information
        id: check
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          echo "::group::Determining PR and branch"

          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            # Manual trigger
            PR_NUMBER="${{ github.event.inputs.pr-number }}"
            PR_JSON=$(gh pr view ${PR_NUMBER} --repo ${{ github.repository }} --json headRefName,labels)
            BRANCH_NAME=$(echo "${PR_JSON}" | jq -r '.headRefName')
          else
            # Automatic trigger from workflow_run
            BRANCH_NAME="${{ github.event.workflow_run.head_branch }}"

            # Find PR for this branch
            PR_JSON=$(gh pr list \
              --repo ${{ github.repository }} \
              --head "${BRANCH_NAME}" \
              --json number,labels \
              --jq '.[0]')

            if [[ "${PR_JSON}" == "null" || -z "${PR_JSON}" ]]; then
              echo "[Fixer] ‚ùå No PR found for branch ${BRANCH_NAME}"
              echo "should-fix=false" >> $GITHUB_OUTPUT
              exit 0
            fi

            PR_NUMBER=$(echo "${PR_JSON}" | jq -r '.number')
          fi

          # Use jq for exact label matching (avoid substring matches)
          LABELS=$(echo "${PR_JSON}" | jq -r '.labels[].name' | tr '\n' ', ' | sed 's/, $//')
          HAS_CONFLICT_RESOLUTION=$(echo "${PR_JSON}" | jq '[.labels[].name] | any(. == "conflict-resolution")')
          HAS_AUTOMATED=$(echo "${PR_JSON}" | jq '[.labels[].name] | any(. == "automated")')
          HAS_UPSTREAM_SYNC=$(echo "${PR_JSON}" | jq '[.labels[].name] | any(. == "upstream-sync")')

          echo "[Fixer] Branch: ${BRANCH_NAME}"
          echo "[Fixer] PR: #${PR_NUMBER}"
          echo "[Fixer] Labels: ${LABELS}"

          # Only fix PRs with upstream sync related labels (exact match)
          if [[ "${HAS_CONFLICT_RESOLUTION}" != "true" && "${HAS_AUTOMATED}" != "true" && "${HAS_UPSTREAM_SYNC}" != "true" ]]; then
            echo "[Fixer] ‚ùå PR doesn't have required labels (need: conflict-resolution, automated, or upstream-sync) - skipping"
            echo "should-fix=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Only fix upstream sync related branches
          if [[ ! "${BRANCH_NAME}" =~ ^upstream-sync-resolver/ && ! "${BRANCH_NAME}" =~ ^upstream-sync/ ]]; then
            echo "[Fixer] ‚ùå Not an upstream sync branch (need: upstream-sync-resolver/* or upstream-sync/*) - skipping"
            echo "should-fix=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "should-fix=true" >> $GITHUB_OUTPUT
          echo "pr-number=${PR_NUMBER}" >> $GITHUB_OUTPUT
          echo "branch-name=${BRANCH_NAME}" >> $GITHUB_OUTPUT

          echo "::endgroup::"

  fix-ci-failures:
    name: Fix CI Failures with Claude
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: check-ci-status
    if: needs.check-ci-status.outputs.should-fix == 'true'

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ needs.check-ci-status.outputs.branch-name }}
          fetch-depth: 0
          token: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}

      - name: Configure git
        run: |
          git config --global user.name "Cartofante"
          git config --global user.email "cartofante@carto.com"

      - name: Get failed workflow details
        id: failures
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          echo "::group::Analyzing CI failures"

          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"

          # Get PR head SHA
          HEAD_SHA=$(gh pr view ${PR_NUMBER} --repo ${{ github.repository }} --json headRefOid --jq '.headRefOid')

          # Get all failed workflow runs for this commit
          gh run list \
            --repo ${{ github.repository }} \
            --commit ${HEAD_SHA} \
            --status failure \
            --json databaseId,name,url \
            --limit 10 > /tmp/failed_runs.json

          FAILED_COUNT=$(cat /tmp/failed_runs.json | jq 'length')

          echo "[Fixer] Found ${FAILED_COUNT} failed workflow runs"
          cat /tmp/failed_runs.json | jq -r '.[] | "  - \(.name): \(.url)"'

          # Get check runs details (gh pr checks uses 'state' not 'conclusion')
          gh pr checks ${PR_NUMBER} \
            --repo ${{ github.repository }} \
            --json name,state,link \
            > /tmp/check_runs.json

          FAILED_CHECKS=$(cat /tmp/check_runs.json | jq '[.[] | select(.state == "FAILURE" or .state == "TIMED_OUT")]')
          echo "${FAILED_CHECKS}" > /tmp/failed_checks.json

          echo "[Fixer] Failed checks:"
          echo "${FAILED_CHECKS}" | jq -r '.[] | "  - \(.name): \(.state)"'

          echo "failed-count=${FAILED_COUNT}" >> $GITHUB_OUTPUT
          echo "head-sha=${HEAD_SHA}" >> $GITHUB_OUTPUT

          echo "::endgroup::"

      - name: Extract CI errors from failed runs
        id: extract-errors
        if: steps.failures.outputs.failed-count != '0'
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          echo "::group::Extracting actual CI errors"

          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"
          BRANCH="${{ needs.check-ci-status.outputs.branch-name }}"

          # Initialize error file
          echo "# CI ERRORS - FIX THESE" > /tmp/all_errors.txt
          echo "" >> /tmp/all_errors.txt

          # Extract Docker Build errors - get the FULL failed job log
          echo "## Docker Build Errors" >> /tmp/all_errors.txt
          DOCKER_RUN=$(gh run list --repo ${{ github.repository }} \
            --workflow="CARTO - Deploy Docker Image (CI)" \
            --branch="${BRANCH}" \
            --status=failure \
            --json databaseId \
            --jq '.[0].databaseId' 2>/dev/null || echo "")

          if [ -n "$DOCKER_RUN" ] && [ "$DOCKER_RUN" != "null" ]; then
            echo "[CI Fixer] Docker run ID: $DOCKER_RUN"
            # Get FULL failed job log
            gh run view $DOCKER_RUN --repo ${{ github.repository }} --log-failed 2>&1 > /tmp/docker_full_log.txt
            DOCKER_LINES=$(wc -l < /tmp/docker_full_log.txt)
            echo "[CI Fixer] Docker log: ${DOCKER_LINES} lines"
            echo "Docker Build Run ID: $DOCKER_RUN (${DOCKER_LINES} lines)" >> /tmp/all_errors.txt
            echo "Full log saved to: /tmp/docker_full_log.txt" >> /tmp/all_errors.txt
          else
            echo "No failed Docker build found" >> /tmp/all_errors.txt
            touch /tmp/docker_full_log.txt
          fi

          # Extract Mock Tests errors - get the FULL failed job log
          echo "" >> /tmp/all_errors.txt
          echo "## Mock Tests Errors" >> /tmp/all_errors.txt
          TESTS_RUN=$(gh run list --repo ${{ github.repository }} \
            --workflow="LiteLLM Mock Tests (folder - tests/test_litellm)" \
            --branch="${BRANCH}" \
            --status=failure \
            --json databaseId \
            --jq '.[0].databaseId' 2>/dev/null || echo "")

          if [ -n "$TESTS_RUN" ] && [ "$TESTS_RUN" != "null" ]; then
            echo "[CI Fixer] Tests run ID: $TESTS_RUN"
            # Get FULL failed job log
            gh run view $TESTS_RUN --repo ${{ github.repository }} --log-failed 2>&1 > /tmp/tests_full_log.txt
            TESTS_LINES=$(wc -l < /tmp/tests_full_log.txt)
            echo "[CI Fixer] Tests log: ${TESTS_LINES} lines"
            echo "Mock Tests Run ID: $TESTS_RUN (${TESTS_LINES} lines)" >> /tmp/all_errors.txt
            echo "Full log saved to: /tmp/tests_full_log.txt" >> /tmp/all_errors.txt
          else
            echo "No failed Mock Tests found" >> /tmp/all_errors.txt
            touch /tmp/tests_full_log.txt
          fi

          # Show extracted errors
          echo "=== All Extracted Errors ==="
          cat /tmp/all_errors.txt

          echo "::endgroup::"

      - name: Comment on PR - Starting Fix
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"

          FAILED_CHECKS=$(cat /tmp/failed_checks.json | jq -r '.[] | "- **\(.name)**: \(.state)"' | head -10)

          gh pr comment ${PR_NUMBER} \
            --repo ${{ github.repository }} \
            --body "$(cat <<EOF
          ## üîß CI Auto-Fix Started

          **Status:** ‚è≥ In progress...

          Claude Code (Opus 4.5) is analyzing and fixing CI failures.

          **Failed Checks:**
          ${FAILED_CHECKS}

          | Step | Status |
          |------|--------|
          | üîç Analyze failures | In progress |
          | ‚úèÔ∏è Apply fixes | Pending |
          | üìå Push fixes | Pending |

          > [!NOTE]
          > This may take **15-30 minutes**. Fixes will be pushed directly to this PR.

          [View workflow run ‚Üí](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          EOF
          )"

      - name: Setup GCP credentials for Vertex AI
        run: |
          echo '${{ secrets.CI_RESOURCES_SERVICE_ACCOUNT }}' > /tmp/gcp-sa.json
          echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-sa.json" >> $GITHUB_ENV

      - name: Fetch PR comments for context
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          echo "::group::Fetching PR comments for previous fix context"

          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"

          # Fetch all PR comments (includes previous fix attempts, human feedback)
          gh pr view ${PR_NUMBER} \
            --repo ${{ github.repository }} \
            --json comments \
            --jq '.comments[] | "---\n**\(.author.login)** at \(.createdAt):\n\(.body)\n"' \
            > /tmp/pr_comments.md 2>/dev/null || echo "No comments found" > /tmp/pr_comments.md

          COMMENT_COUNT=$(gh pr view ${PR_NUMBER} --repo ${{ github.repository }} --json comments --jq '.comments | length' 2>/dev/null || echo "0")
          echo "[CI Fixer] Found ${COMMENT_COUNT} PR comments for context"

          echo "::endgroup::"

      # =========================================================================
      # Run CARTO PR Analysis (generates file list on-demand)
      # =========================================================================
      - name: Run CARTO PR analysis
        id: carto-analysis
        uses: ./.github/actions/carto-pr-analysis
        with:
          github_token: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
          output_path: /tmp/carto-analysis

      - name: Run Claude Code to fix failures
        id: claude-fix
        uses: anthropics/claude-code-action@v1
        with:
          use_vertex: "true"
          github_token: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
          show_full_output: true
          claude_args: "--model claude-opus-4-5@20251101 --max-turns 200 --allowedTools Read,Write,Edit,Bash,Grep,Glob"
          prompt: |
            # CI Failure Auto-Fix Session

            **PR:** #${{ needs.check-ci-status.outputs.pr-number }}
            **Branch:** `${{ needs.check-ci-status.outputs.branch-name }}`

            ---

            ## üö® IMPORTANT: HOW THIS WORKS

            **DO NOT wait for CI to run.** Your job is simple:

            1. **READ** the CI failure logs (already extracted for you)
            2. **UNDERSTAND** the error by reading the relevant code
            3. **PLAN** a fix
            4. **IMPLEMENT** the fix
            5. **PUSH** and **END SESSION**

            **FEEDBACK LOOP:** After you push, CI will run automatically. If it fails again,
            this workflow triggers a NEW session with the NEW error logs. You don't wait -
            each session is one fix attempt.

            ---

            ## üéØ PRIORITIES (in order)

            1. **FIX CI** - The main goal is making CI pass
            2. **PRESERVE BOTH** - Try to keep both CARTO customizations AND upstream code working together
            3. **MERGE INTELLIGENTLY** - If there's a conflict between CARTO and upstream, find a solution that keeps both
            4. **LAST RESORT** - Only revert CARTO changes or skip tests if no other option exists

            ---

            ## üìã CARTO-MODIFIED FILES REFERENCE

            **Check if failing files are CARTO-modified:**

            ```bash
            # List of files modified by CARTO PRs (preserve these changes!)
            if [ -f /tmp/carto-analysis/carto_files.txt ]; then
              echo "CARTO-modified files ($(wc -l < /tmp/carto-analysis/carto_files.txt) total):"
              head -50 /tmp/carto-analysis/carto_files.txt
            else
              echo "No CARTO analysis available"
            fi

            # Check if a specific failing file is CARTO-modified:
            # grep "path/to/failing/file.py" /tmp/carto-analysis/carto_files.txt
            ```

            **Decision matrix:**
            | File in CARTO list? | Fix Strategy |
            |---------------------|--------------|
            | YES | Preserve CARTO code, fix around it |
            | NO | Accept upstream, fix imports/signatures |

            ---

            ## STEP 1: READ THE CI FAILURE LOGS

            The CI logs have been pre-extracted. **Read them completely:**

            ```bash
            echo "============================================"
            echo "=== STEP 1: READING CI FAILURE LOGS ==="
            echo "============================================"

            echo ""
            echo "--- DOCKER BUILD LOG (last 500 lines - errors at end) ---"
            echo "Full log: /tmp/docker_full_log.txt ($(wc -l < /tmp/docker_full_log.txt) lines)"
            echo ""
            tail -500 /tmp/docker_full_log.txt

            echo ""
            echo "--- MOCK TESTS LOG (Python errors) ---"
            cat /tmp/tests_full_log.txt
            ```

            **IMPORTANT:** Read these logs carefully. Extract:
            - The exact error message
            - The file path and line number
            - The context around the error

            ---

            ## STEP 2: READ THE FAILING CODE

            After identifying the error location from the logs, **read the actual source file:**

            ```bash
            # Example: If error is in ui/litellm-dashboard/src/components/Foo.tsx:42
            cat ui/litellm-dashboard/src/components/Foo.tsx

            # Example: If error is ImportError for 'some_function' in litellm/proxy/utils.py
            cat litellm/proxy/utils.py
            ```

            **Understand the code context before making changes.**

            ---

            ## STEP 3: CHECK WHAT UPSTREAM AND CARTO HAVE (for merge issues)

            If the error is due to missing code, modules, or merge conflicts:

            ```bash
            # Fetch both remotes - IMPORTANT!
            git fetch origin carto/main
            git remote add upstream https://github.com/BerriAI/litellm.git 2>/dev/null || true
            git fetch upstream main

            # See what carto/main has
            git show origin/carto/main:path/to/file.py 2>/dev/null | head -100

            # See what upstream has
            git show upstream/main:path/to/file.py 2>/dev/null | head -100

            # Compare folder contents across all three branches
            echo "=== CARTO/MAIN ==="
            git ls-tree --name-only origin/carto/main path/to/folder/
            echo "=== UPSTREAM/MAIN ==="
            git ls-tree --name-only upstream/main path/to/folder/
            echo "=== CURRENT BRANCH ==="
            git ls-tree --name-only HEAD path/to/folder/
            ```

            ---

            ## üîç STEP 3.5: TROUBLESHOOTING - COMPARE BRANCH STATES

            **CRITICAL:** For "Cannot find module" or "ImportError" errors, ALWAYS compare branches first!

            | Error Pattern | Root Cause | Fix Location |
            |--------------|------------|--------------|
            | `Type error: Expected X arguments` | TypeScript function signature mismatch | `ui/litellm-dashboard/src/` |
            | `Cannot find module` | Missing import in UI | `ui/litellm-dashboard/src/` |
            | `Failed to compile` | Next.js build error | `ui/litellm-dashboard/src/` |
            | `ImportError` / `ModuleNotFoundError` | Python import missing | `litellm/` or `tests/` |
            | `SyntaxError` | Conflict markers or bad merge | Check for `<<<<<<<` markers |
            | `FAILED tests/...` | Test assertion failure | `tests/` - fix test expectations |
            | `AssertionError` | Test expecting wrong value | Update test to match CARTO behavior |
            | `'Prisma' object has no attribute` | **schema.prisma missing tables** | Sync `schema.prisma` from upstream |
            | `AttributeError: 'Prisma'` | **schema.prisma out of sync** | Sync `schema.prisma` from upstream |

            ### Decision Matrix for Missing Files

            | Scenario | Fix |
            |----------|-----|
            | File exists on carto/main but NOT on sync branch | **Restore it:** `git checkout origin/carto/main -- <path>` |
            | File exists on upstream with DIFFERENT path | **Update import:** Change import to use new path |
            | File was renamed in upstream | **Update all imports** to use new name |
            | CARTO file depends on missing CARTO file | **Restore CARTO dependencies** from origin/carto/main |

            ### Example: Restoring Missing TypeScript Files

            If error is "Cannot find module './chat_ui/llm_calls/fetch_models'":

            ```bash
            # Check what carto/main has in that folder
            git ls-tree origin/carto/main ui/litellm-dashboard/src/components/chat_ui/llm_calls/

            # If file exists on carto/main but missing here, restore it:
            git checkout origin/carto/main -- ui/litellm-dashboard/src/components/chat_ui/llm_calls/fetch_models.tsx

            # Or restore the entire folder if multiple files are missing:
            git checkout origin/carto/main -- ui/litellm-dashboard/src/components/chat_ui/llm_calls/
            ```

            ### Example: Restoring Missing Python Functions

            If error is "ImportError: cannot import name 'some_function'":

            ```bash
            # Check if carto/main has this function
            git show origin/carto/main:litellm/proxy/utils.py | grep -A 20 "def some_function"

            # If it exists, restore the file from carto/main
            git checkout origin/carto/main -- litellm/proxy/utils.py
            ```

            ### For Schema/Prisma Errors

            If you see errors like:
            - `'Prisma' object has no attribute 'litellm_agentstable'`
            - `AttributeError: 'Prisma' object has no attribute`

            This means `schema.prisma` is missing tables from upstream. **This is a CRITICAL issue.**

            **Fix:**
            ```bash
            # Check current vs upstream table count
            echo "Current tables: $(grep -c '^model ' schema.prisma)"
            echo "Upstream tables: $(git show upstream/main:schema.prisma | grep -c '^model ')"

            # Sync the entire schema from upstream
            git show upstream/main:schema.prisma > schema.prisma

            # Or from the current branch's origin (which should have upstream content)
            git show origin/HEAD:schema.prisma > schema.prisma
            ```

            **Required tables that MUST exist:**
            - LiteLLM_AgentsTable
            - LiteLLM_SSOConfig
            - LiteLLM_CacheConfig
            - LiteLLM_ManagedVectorStoreIndexTable
            - LiteLLM_UISettings
            - LiteLLM_SkillsTable

            ---

            ## STEP 4: PLAN YOUR FIX

            Before coding, decide on the approach:

            | Error Type | Best Fix Approach |
            |------------|-------------------|
            | **Cannot find module** | Check if file exists on carto/main ‚Üí Restore with `git checkout origin/carto/main -- <path>` |
            | **ImportError (Python)** | Check if function exists on carto/main ‚Üí Restore file or add function |
            | Missing function/class | Restore from carto/main OR upstream, whichever has it |
            | TypeScript type mismatch | Update call site to match function signature |
            | Import error | Add the missing import statement |
            | Merge conflict markers | Remove markers, merge both versions intelligently |
            | Test assertion failure | Update test OR fix code to match expected behavior |
            | **Schema/Prisma error** | Sync `schema.prisma` from upstream (see above) |

            ---

            ## STEP 5: IMPLEMENT THE FIX

            Make the necessary code changes using Edit/Write tools.

            **For TypeScript errors:** UI code is in `ui/litellm-dashboard/src/`
            **For Python errors:** Main code in `litellm/`, tests in `tests/`

            ---

            ## STEP 6: COMMIT AND PUSH (then END SESSION)

            **CRITICAL:** All CI fixes MUST be in ONE commit. Always amend if a previous CI fix exists.

            ```bash
            git add -A
            git status

            # Check if last commit was a CI fix by Cartofante (to amend it)
            LAST_COMMIT_MSG=$(git log -1 --pretty=%s)
            LAST_COMMIT_AUTHOR=$(git log -1 --pretty=%an)

            echo "Last commit: '$LAST_COMMIT_MSG' by '$LAST_COMMIT_AUTHOR'"

            if [[ "$LAST_COMMIT_MSG" == fix\(ci\):* ]] && [[ "$LAST_COMMIT_AUTHOR" == "Cartofante" ]]; then
              echo "‚úÖ Found previous CI fix commit - AMENDING..."
              git commit --amend --no-edit
              git push --force-with-lease origin ${{ needs.check-ci-status.outputs.branch-name }}
              echo "Amended and pushed successfully"
            else
              echo "üìù No previous CI fix commit - creating new one..."
              git commit -m "fix(ci): resolve CI failures from upstream sync

            Automated fix by Claude Code.
            "
              git push origin ${{ needs.check-ci-status.outputs.branch-name }}
              echo "Committed and pushed successfully"
            fi
            ```

            **After pushing, your session is DONE.** CI will run, and if it fails again,
            a new session will start with the new error logs. All fixes will be amended into ONE commit.

            ---

            ## STEP 7: COMMENT ON PR

            ```bash
            gh pr comment ${{ needs.check-ci-status.outputs.pr-number }} --repo ${{ github.repository }} --body "## üîß CI Fix Applied

            **Error Fixed:**
            \`\`\`
            [PASTE EXACT ERROR]
            \`\`\`

            **Fix Applied:**
            - File: \`[FILE PATH]\`
            - Change: [DESCRIPTION]

            **Verification:** Pushed to branch, CI will re-run automatically.

            ---
            _Attempt #[N] - If this fails, I'll analyze the new errors and try again._"
            ```

            ---

            ## DECISION TREE

            ```
            START
              ‚îÇ
              ‚îú‚îÄ‚ñ∫ Check PR status (gh pr checks)
              ‚îÇ
              ‚îú‚îÄ‚ñ∫ Is Docker Build failing?
              ‚îÇ     ‚îÇ
              ‚îÇ     ‚îî‚îÄ‚ñ∫ YES: Get Docker logs ‚Üí Find TypeScript error ‚Üí Fix in ui/litellm-dashboard/
              ‚îÇ
              ‚îú‚îÄ‚ñ∫ Are Mock Tests failing?
              ‚îÇ     ‚îÇ
              ‚îÇ     ‚îî‚îÄ‚ñ∫ YES: Get test logs ‚Üí Find failing test ‚Üí Fix test expectations (not CARTO code)
              ‚îÇ
              ‚îú‚îÄ‚ñ∫ Are there conflict markers?
              ‚îÇ     ‚îÇ
              ‚îÇ     ‚îî‚îÄ‚ñ∫ YES: Remove markers, keep CARTO version
              ‚îÇ
              ‚îî‚îÄ‚ñ∫ Commit & Push ‚Üí CI re-runs ‚Üí If fails, this workflow runs again
            ```

            ---

            ## EXAMPLES

            ### Example 1: TypeScript Function Signature Error

            **Error:** `Type error: Expected 0 arguments, but got 1`
            **File:** `ui/litellm-dashboard/src/.../PriceDataManagementTab.tsx:28`
            **Code:** `const data = await modelCostMap(accessToken);`

            **Fix Process:**
            1. Find function definition: `grep -rn "modelCostMap" ui/litellm-dashboard/src/`
            2. Check signature: `export const modelCostMap = async (): Promise<...>`
            3. Fix call: Remove the argument ‚Üí `const data = await modelCostMap();`

            ### Example 2: Python Test Assertion Error

            **Error:** `AssertionError: assert 'old_value' == 'new_carto_value'`
            **File:** `tests/test_something.py::test_feature`

            **Fix Process:**
            1. CARTO intentionally changed the behavior
            2. Update test: `assert result == 'new_carto_value'`
            3. Add comment: `# CARTO: Updated for new behavior`

            ---

            ## üöÄ NOW START!

            **Execute these steps in order:**

            1. **READ LOGS** - `cat /tmp/docker_full_log.txt` and `cat /tmp/tests_full_log.txt`
            2. **READ CODE** - Read the files mentioned in the error
            3. **UNDERSTAND** - What's the root cause?
            4. **FIX** - Make the code change
            5. **PUSH** - Commit and push
            6. **DONE** - End session. CI will run. If it fails, new session starts.

            **Remember:** You are NOT waiting for CI. Push your fix and end the session.
        env:
          ANTHROPIC_VERTEX_PROJECT_ID: carto-ci-resources
          CLOUD_ML_REGION: global
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
          GITHUB_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}

      - name: Cleanup GCP credentials
        if: always()
        run: rm -f /tmp/gcp-sa.json

      - name: Comment on PR - Result
        if: always()
        env:
          GH_TOKEN: ${{ secrets.X_GITHUB_SUPERCARTOFANTE }}
        run: |
          PR_NUMBER="${{ needs.check-ci-status.outputs.pr-number }}"

          if [ "${{ steps.claude-fix.outcome }}" == "success" ]; then
            gh pr comment ${PR_NUMBER} \
              --repo ${{ github.repository }} \
              --body "$(cat <<EOF
          ## ‚úÖ CI Fix Complete

          Fixes applied and pushed to this PR.

          | Step | Status |
          |------|--------|
          | üîç Analyze failures | ‚úÖ Complete |
          | ‚úèÔ∏è Apply fixes | ‚úÖ Complete |
          | üìå Push fixes | ‚úÖ Complete |

          > [!NOTE]
          > CI will re-run automatically. Monitor check results below.

          ### Next Steps

          1. Wait for CI checks to complete
          2. If failures persist, manual review may be needed
          3. Merge when all checks pass

          <details>
          <summary>üîß Workflow Details (click to expand)</summary>

          **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}

          </details>
          EOF
          )"
          else
            gh pr comment ${PR_NUMBER} \
              --repo ${{ github.repository }} \
              --body "$(cat <<EOF
          ## ‚ùå CI Fix Failed

          The automated fix encountered an error.

          | Step | Status |
          |------|--------|
          | üîç Analyze failures | ${{ steps.claude-fix.outcome }} |
          | ‚úèÔ∏è Apply fixes | - |
          | üìå Push fixes | - |

          > [!WARNING]
          > Manual intervention required. Check the workflow logs below.

          <details>
          <summary>üîß Troubleshooting (click to expand)</summary>

          **Common issues:**
          - Complex merge conflicts requiring human judgment
          - Multiple interrelated failures
          - Test failures requiring code logic changes

          **Manual fix steps:**
          1. Check the workflow logs
          2. Review error messages
          3. Fix issues locally and push

          </details>

          [View workflow logs ‚Üí](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          EOF
          )"
          fi
